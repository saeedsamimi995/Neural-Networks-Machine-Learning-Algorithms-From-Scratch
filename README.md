# Neural Networks & Machine Learning Algorithms â€” From Scratch

This repository contains a collection of Jupyter Notebook (`.ipynb`) implementations of core machine learning and neural network models **built entirely from scratch** using vanilla Python and NumPy.  
The purpose of these implementations is to provide a deep, fundamental understanding of how these algorithms work internally, without relying on high-level libraries such as TensorFlow, Keras, or PyTorch.

## ğŸ“˜ Contents

The repository includes manual implementations of:

### ğŸ”· Multi-Layer Perceptron (MLP)
- Forward and backward propagation
- Manual gradient computation
- Activation functions and custom loss definitions
- Training loops built step-by-step

### ğŸ”· Support Vector Machine (SVM)
- Hard-margin and soft-margin intuition
- Implementation using gradient-based optimization or quadratic programming
- Visualization of decision boundaries (where applicable)

### ğŸ”· Autoencoders
- Encoderâ€“decoder architecture implemented from scratch
- Backpropagation written manually
- Reconstruction error analysis and visualization

## ğŸ“ Academic Context

These notebooks are practice material developed as part of **Neural Networks (NN)** and **Deep Learning (DL)** academic coursework.  
They aim to reinforce theoretical concepts by re-building the algorithms in a low-level, transparent manner.

## ğŸ› ï¸ Technologies Used

- Python
- NumPy
- Matplotlib
- Jupyter Notebook

## ğŸ“‚ Project Structure

